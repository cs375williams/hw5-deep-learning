{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cc95f30",
   "metadata": {},
   "source": [
    "# Homework 5: Deep Learning for NLP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bfca0",
   "metadata": {},
   "source": [
    "The goals of this assignment are: \n",
    "1. Understand why deep learning can be beneficial for NLP classification tasks\n",
    "2. Implement a simple computation graph *by hand*\n",
    "3. Implement and train a deep averaging network (DAN) with real-world data and Pytorch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e05e52d",
   "metadata": {},
   "source": [
    "## Organization and Instructions\n",
    "Execute the code cells in Part 1 to understand the background for this assignment. You will not need to modify or add anything to Part 1. Part 2 is where your solution begins.\n",
    "\n",
    "**Part 1: Background.** \n",
    "- 1A. Environment set-up \n",
    "- 1B. Data exploration \n",
    "\n",
    "**Part 2: Your implementation.** \n",
    "- 2A. Limitations of linear models \n",
    "- 2B. Computation graphs \n",
    "- 2C. Leaky ReLUs\n",
    "- 2D. Deep averaging networks  \n",
    "\n",
    "**(Optional) Part 3: Extra Credit.** \n",
    "Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.\n",
    "\n",
    "**Addtional instructions.** \n",
    "- Your submitted solution and code must be yours alone. Copying and pasting a solution from the internet or another source is considered a violation of the honor code. \n",
    "\n",
    "**Evaluation.** Your solution will be evaluated on a mix of: \n",
    "- Unit tests\n",
    "- Accuracy on the held-out test set. \n",
    "- Manually-graded free response questions. \n",
    "\n",
    "**Grading.**\n",
    "\n",
    "- Part 2A.\n",
    "    - **5 points (autograded).** Unit test that your `train_logistic_regression_sklearn()` function is obtaining the correct accuracy. \n",
    "    - **2 points (manually graded).** We will manually grade your answer to the free response question for 2A. \n",
    "- Part 2B. \n",
    "    - **5 points (autograded).** Unit test that your `f_forward_backward()` function is getting the correct results. \n",
    "- Part 2C.\n",
    "    - **5 points (autograded).** Unit test that your `leaky_relu_forward_backward()` function is getting the correct results. \n",
    "    - **2 points (manually graded).** We will manually grade your answer to the free response question for 2C.\n",
    "- Part 2D.\n",
    "    - **5 points (autograded).** Unit tests for `create_word_indices()`.\n",
    "    - **2.5 points (autograded).** Unit tests for `truncate()`. \n",
    "    - **2.5 points (autograded).** Unit tests for `pad()`.\n",
    "    - **10 points (autograded).** Accuracy on the Yelp training set. \n",
    "    - **10 points (autograded).** Accuracy on the Yelp validation set. \n",
    "    - **10 points (autograded).** Accuracy on the Yelp test set.\n",
    "    \n",
    "For all the accuracy portions in Part 2D, you aim is to get at least 0.51 accuracy. For each `split`, your points will be calculated as \n",
    "\n",
    "```\n",
    "(1 -(0.51 - min(accuracy on split, 0.51))/0.51) * 10 points \n",
    "```\n",
    "\n",
    "This is a very generous accuracy requirement and we encourage you to work on the extra credit to challenge yourself to get a higher accuracy on this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f28832",
   "metadata": {},
   "source": [
    "### 1A. Environment Set-up "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0c8ca2",
   "metadata": {},
   "source": [
    "If you set-up your conda environment correctly in HW0, you should see `Python [conda env:cs375]` as the kernel in the upper right-hand corner of the Jupyter webpage you are currently on. Run the cell below to make sure your environment is correctly installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c48f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment check \n",
    "# Return to HW0 if you run into errors in this cell \n",
    "# Do not modify this cell \n",
    "import os\n",
    "assert os.environ['CONDA_DEFAULT_ENV'] == \"cs375\"\n",
    "\n",
    "import sys\n",
    "assert sys.version_info.major == 3 and sys.version_info.minor == 11"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77f509",
   "metadata": {},
   "source": [
    "If there are any errors after running the cell above, return to the instructions from `HW0`. If you are still having difficulty, reach out to the instructor or TAs via Piazza. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4da7c6a",
   "metadata": {},
   "source": [
    "#### Installing Pytorch \n",
    "We tested the following Pytorch installation for Macs. For Windows users, we may need to find a workaround together. \n",
    "\n",
    "Steps: \n",
    "1. Open a terminal. \n",
    "2. Activate your cs375 environment via the following terminal command:\n",
    "    ```\n",
    "    conda activate cs375\n",
    "    ```\n",
    "3. Install pytorch by typing via the following terminal command:\n",
    "    ```\n",
    "    conda install pytorch \n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2067524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this cell to make sure you implemented the steps above correctly\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b521bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pytorch version =', torch.__version__)\n",
    "# If you installed correctly with conda in HW0 you should see 2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d637bf",
   "metadata": {},
   "source": [
    "#### Installing other packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b3aefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules for this assignment \n",
    "# Do not modify this cell \n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Tuple\n",
    "import sklearn \n",
    "from sklearn import linear_model\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import matplotlib.pyplot as plt\n",
    "import util #check out util.py in this repository for the helper functions we've implemented for you "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ee52cb",
   "metadata": {},
   "source": [
    "### 1B. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92e8798",
   "metadata": {},
   "source": [
    "#### Toy data \n",
    "We will use the following toy data for a few questions in this assignement. \n",
    "\n",
    "Here, the rows in X are examples but the columns in X are *not* words like we've seen so far. Instead, these columns are two *features*: \n",
    "- Average sentiment of words in a sentence \n",
    "- Sarcasm level in sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cd2ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to construct and plot this toy data \n",
    "X_toy, y_toy = util.generate_2d_xor_dataset()\n",
    "plt = util.plot_2d_dataset_points(X_toy, y_toy);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6630a36",
   "metadata": {},
   "source": [
    "#### Yelp data \n",
    "\n",
    "Later in this assignment you will use a dataset of real-world Yelp reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f599787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load Yelp data\n",
    "train_examples, train_labels = util.load_dataset(\"./data/yelp/train.csv\")\n",
    "dev_examples, dev_labels = util.load_dataset(\"./data/yelp/dev.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7ce9de",
   "metadata": {},
   "source": [
    "Each element in `train_examples` is a Yelp review that we've preprocessed into a list of words. We've already removed punctuation, segmented the words, and converted them all to lower case for you, to make them easier to work with.\n",
    "\n",
    "Each element in `train_labels` is the review's corresponding positive/negative label. We've transformed the original star ratings into two labels (0 or 1) by collapsing 1 and 2-star reviews into the negative class (label 0) and 4 and 5-star reviews into the positive class (label 1). We ignore 3-star reviews for simplicity.\n",
    "\n",
    "Here, `dev_examples` and `dev_labels` are a separate development set with the exact same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f5c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at a single training example\n",
    "\" \".join(train_examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd1808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at that example's label\n",
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another\n",
    "\" \".join(train_examples[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf56bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another\n",
    "train_labels[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07013b71",
   "metadata": {},
   "source": [
    "### 2A. Limitations of linear models \n",
    "The goals of this part are \n",
    "- To understand the limitations of linear models like Logistic Regression. \n",
    "- To practice using `sklearn`, a package which implements many machine learning models like Logistic Regression. Many of you will likely want to use `sklearn` for your final projects.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e4c9cf",
   "metadata": {},
   "source": [
    "Below, we'll use sklearn's Logistic Regression model \n",
    "```\n",
    "logistic_regression_classifier = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "```\n",
    "\n",
    "Read more about this model [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    "\n",
    "In your implementation of the `train_logistic_regression_sklearn()` function below, chose among the following methods from the `sklearn` documentation to train your logistic regression classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd9c69a",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./imgs/logreg-methods.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83288c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_logistic_regression_sklearn(X: np.ndarray, y: np.ndarray) -> Tuple[sklearn.linear_model._logistic.LogisticRegression, float]:\n",
    "    \"\"\"\n",
    "    Arguments: \n",
    "        - X (np.ndarray): Rows are examples, columns are \"features\", \n",
    "                        e.g., in our toy data above, one feature is average sentiment \n",
    "                        and the other feature is sarcasm level in the sentence. \n",
    "        - y (np.ndarray): Each element is a class label corresponding to the example in X \n",
    "                        e.g., in our toy data above,  \n",
    "        \n",
    "    Returns: As a tuple with \n",
    "            - First element of tuple: trained sklearn model object \n",
    "            - Second element of tuple: accuracy on the training set (float) \n",
    "        \n",
    "    Hints: \n",
    "        - This function should be pretty simple if you correctly choose and utilize the sklearn Logistic Regresion\n",
    "        methods from the screen shot above \n",
    "        - Make sure to check the type of the accuracy you return \n",
    "    \"\"\"\n",
    "    # Check to make sure inputs are correct \n",
    "    assert X.shape[0] == y.shape[0]\n",
    "    \n",
    "    # Intialize the sklearn model (no need to modify)\n",
    "    logistic_regression_classifier = sklearn.linear_model.LogisticRegression(penalty=None)\n",
    "\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    pass #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2cdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We reshape y_toy slightly to correspond to the form sklearn requires \n",
    "# NO NEED TO MODIFY THIS CELL \n",
    "print('y_toy original shape =', y_toy.shape)\n",
    "y_toy_reshape = y_toy.reshape(-1) # reshape \n",
    "print('new shape =', y_toy_reshape.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef263b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your function to train on toy data \n",
    "# NO NEED TO MODIFY THIS CELL \n",
    "clf, acc = train_logistic_regression_sklearn(X_toy, y_toy_reshape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfb7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type checks \n",
    "assert type(clf) == linear_model._logistic.LogisticRegression\n",
    "assert type(acc) == float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2485c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy of logistic regression classifier on toy data = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9b2253",
   "metadata": {},
   "source": [
    "Below, we have provided you with a function that shades the predictions your trained classifier makes. In other words, the area shaded red is where your trained classifier predicts positive sentiment labels and the area shaded blue is where your trained classifier predicts negative sentiment labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3ac79",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = util.plot_points_with_classifier_predictions(X_toy, y_toy, clf);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a683ed",
   "metadata": {},
   "source": [
    "#### (2 points) Manual response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92750d8c",
   "metadata": {},
   "source": [
    "Why is the Logistic Regression classifier you trained above performing poorly? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac80225",
   "metadata": {},
   "source": [
    "*DELETE AND PUT YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374d36bd",
   "metadata": {},
   "source": [
    "### 2B. Computation graphs\n",
    "\n",
    "Recall from lecture, *computation graphs* are visual representations of functions. Nodes in the graph are variables. For each node, we calculate both the (1) output value and (2) local gradient with respect to its output at each node. \n",
    "\n",
    "Let's implement the forward and backward pass for the following function: \n",
    "```\n",
    "f(x, y, z) = max(0, x*y) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9d3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_forward_backward(x: float, y: float, z:float) -> dict: \n",
    "    \"\"\"\n",
    "    Implements the forward and backward (gradient) pass \n",
    "    for the following function\n",
    "    \n",
    "    f(x, y, z) = max(0, x*y) \n",
    "    \n",
    "    NOTE: You are only allowed to use numpy in this function.\n",
    "    You are NOT allowed to use torch (or any other package). \n",
    "    \n",
    "    Hints:\n",
    "        - It may be helpful to review the computation graphs we did in lecture.\n",
    "        - It may be helpful for you to work this out on paper first. \n",
    "    \"\"\"\n",
    "    #Initalize with None's what the function will return \n",
    "    result = {\"f_forward\": None, \n",
    "                \"df_dx\": None,\n",
    "                \"df_dy\": None, \n",
    "                \"df_dz\": None}\n",
    "    \n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    pass #delete this line and add your solution\n",
    "    # CODE END \n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the following matches what you calculate on paper \n",
    "f_forward_backward(2, 1, -4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2ab6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the following matches what you calculate on paper \n",
    "f_forward_backward(2, -1, -4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5009bbae",
   "metadata": {},
   "source": [
    "###  2C. Leaky ReLU\n",
    "\n",
    "The function you implemented in Part 2B is very similar to a `ReLU`, one of the nonlinear activation functions we discussed during lecture.  \n",
    "\n",
    "In practice, most people building deep learning architectures use a `LeakyReLU` as the nonlinear activation function. This activation function has a hyperparameter, `negative_slope` that is used when the input is less than 0, and the function takes the following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a65d19",
   "metadata": {},
   "source": [
    "$$ \\text{LeakyReLU}(u) = \\begin{cases}\n",
    "u &\\text{ if } u \\ge 0 \\\\\n",
    "\\text{negative_slope} \\cdot u &\\text{ otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518fc95f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./imgs/LeakyReLU.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0211bb8",
   "metadata": {},
   "source": [
    "![](path-to-image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a2b85",
   "metadata": {},
   "source": [
    "In the function below, implement the forward and backward pass of the `Leaky ReLU`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c6026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu_forward_backward(u:float, negative_slope: float=0.1) -> dict: \n",
    "    \"\"\"\n",
    "    This function implements the Leaky ReLU shown above. \n",
    "    \n",
    "    Arguments: \n",
    "        - u (float): the input value to the Leaky ReLU\n",
    "        - negative_slope (float): the slope for values of u that are less than 0 \n",
    "    \n",
    "    Returns: \n",
    "        - result (dict): results of the forward pass given the input\n",
    "                variable and the partial derivatives of the forward pass with respect\n",
    "                to each input variable \n",
    "                \n",
    "    NOTE: You are only allowed to use numpy in this function.\n",
    "    You are NOT allowed to use torch (or any other package).\n",
    "    \n",
    "    Hints: \n",
    "        - Most likely, you will want to work this out on paper first.  \n",
    "    \"\"\"\n",
    "    result = {\"forward\": None, #result of the forward pass\n",
    "              \"df_du\": None}\n",
    "    \n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    pass #delete this line and add your solution\n",
    "    # CODE END \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a1549",
   "metadata": {},
   "source": [
    "We're not providing you unit tests in this notebook for your LeakyReLU. Try writing some of your own to test your function! While these will not be graded, they'll help you become better at testing code and potentially help you pass the autograder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your own unit tests for LeakyReLU function here. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31783dbc",
   "metadata": {},
   "source": [
    "#### (2 points) Manual response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f2ae10",
   "metadata": {},
   "source": [
    "Why would deep learning architecture designers prefer Leaky ReLU over ReLUs as nonlinear activation functions? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc401ec5",
   "metadata": {},
   "source": [
    "*DELETE AND PUT YOUR ANSWER HERE* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6002b158",
   "metadata": {},
   "source": [
    "### 2D. Deep Averaging Networks in Pytorch \n",
    "\n",
    "In this part, you will implement Deep Averaging Networks, a simple feedforward neural network that uses dense word embeddings as input. Re-read [the paper](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965dff8",
   "metadata": {},
   "source": [
    "Recall, here is the version of DANs we discussed in lecture: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f43e2",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"./imgs/dan.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38da7d9",
   "metadata": {},
   "source": [
    "In the figure above,   \n",
    "- Each $\\vec{v}_i$ is a dense word embedding that thas been pre-trained on another corpus (e.g. Wikipedia). \n",
    "- $g$ is a non-linear activation function. In your implementation, you will use [Leaky ReLUs implemented with Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e882541",
   "metadata": {},
   "source": [
    "A few more notes: \n",
    "- The embeddings are *frozen* meaning the gradients do not pass back through them. \n",
    "- The output of your forward pass should be a *log probability* for numerical stability.\n",
    "- Implement a **dropout layer** (Pytorch documentation [here](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)) after the word embeddings but before the average (the authors call this \"word dropout\"). Also use dropout after calculating both $h_1$ and $h_2$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5c3bec",
   "metadata": {},
   "source": [
    "#### Processing data \n",
    "\n",
    "First, we'll need a little more data wrangling to get the Yelp data into a format for Pytorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c01ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same GloVe embeddings you used for HW 4 \n",
    "# NO NEED TO MODIFY THIS CELL \n",
    "embeddings = KeyedVectors.load_word2vec_format(\"./data/embeddings/glove50_4k.txt\", binary=False)\n",
    "vocab2indx = dict(embeddings.key_to_index)\n",
    "idx2vocab = list(embeddings.index_to_key)\n",
    "embed_array = embeddings.vectors # matrix of dense word embeddings \n",
    "                                 # rows: a word \n",
    "                                 # columns: dimensions (50) of the dense embeddings\n",
    "print(\"Size of embedding vocab = \", len(idx2vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58289f3d",
   "metadata": {},
   "source": [
    "Above, `vocab2idx` above is a dictionary with: \n",
    "- Keys: strings corresponding to words in the vocab\n",
    "- Values: the \"index\" of a word, i.e., an int corresponding to the row index for the embedding in embed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3461fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "king_idx = vocab2indx[\"king\"]\n",
    "king_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b15690b",
   "metadata": {},
   "source": [
    "Below is the embedding (a dense vector) for the word \"king\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16788efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_array[king_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a755b42b",
   "metadata": {},
   "source": [
    "We'll add an out-of-vocabulary (`\"<OOV>\"`) symbol to the vocabulary and initalize it as the vector for \"the\" so it does not affect the embedding average in our deep learning network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba772fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_the_embedding(embed_array, vocab2indx, embedding_dim=50): \n",
    "    \"\"\"\n",
    "    Adds \"the\" embedding to the embed_array matrix\n",
    "    \"\"\"\n",
    "    the_embedding = embed_array[vocab2indx[\"the\"]]\n",
    "    out = np.vstack((embed_array, the_embedding))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abc245f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <OOV> symbol \n",
    "new_oov_entry = len(embeddings)\n",
    "idx2vocab += [\"<OOV>\"]\n",
    "vocab2indx[\"<OOV>\"] = new_oov_entry\n",
    "embed_array_w_oov = add_the_embedding(embed_array, vocab2indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f47cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check we've successfully added a row to our embed_array matrix and an entry to idx2vocab\n",
    "# Hints: if you're getting errors here you may have run the cell with += above too many times \n",
    "    # try restarting your kernel and running again \n",
    "assert embed_array_w_oov.shape == (embed_array.shape[0]+1, embed_array.shape[1])\n",
    "assert len(idx2vocab) == len(embeddings) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f852b6",
   "metadata": {},
   "source": [
    "Now we need to convert tokens into indices in the `embed_array`. Essentially, this will tell your Pytorch implementation which row of `embed_array` to use as the embedding for a word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3116380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_indices(tokens: List[str], vocab2indx: dict) -> List[int]: \n",
    "    \"\"\"\n",
    "    For each example, translate each token into its corresponding index from vocab2indx\n",
    "    \n",
    "    Replace words not in the vocabulary with the symbol \"<OOV>\" \n",
    "        which stands for 'out of vocabulary'\n",
    "        \n",
    "    Arguments: \n",
    "       - tokens (List[str]): list of strings of tokens \n",
    "       - vocab2indx (dict): each vocabulary word as strings and its corresponding int index \n",
    "                           for the embeddings \n",
    "                           \n",
    "    Returns: \n",
    "        - (List[int]): list of integers\n",
    "    \"\"\" \n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unit test 1\n",
    "toks1 = \"this is great\".split()\n",
    "out1 = create_word_indices(toks1, vocab2indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a839e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should get [37, 14, 353]\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310b2b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unit test 2 \n",
    "toks2 = \"this is baloney\".split()\n",
    "out2 = create_word_indices(toks2, vocab2indx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a87eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should get [37, 14, 4196]\n",
    "# Hint if incorrect: make sure you're handling OOVs properly\n",
    "out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de97402",
   "metadata": {},
   "source": [
    "For computational efficiency, Pytorch requires that all inputs in a batch be the same length. Often, different pices of text have different lengths (e.g. different number of words in a sentence) so we must make the following modifications:  \n",
    "- `truncate`: Given a maximum length (set by the you), we truncate the last part of the sequence. While we lose information, with a large enough maximum length, we can still approximate the information in each example/sentence even when it's truncated. \n",
    "- `pad`: Given inputs that are less than the maximum length, we concatenate the input sequence with a special pad token, `\"<PAD>\"`, the number of times it takes to get it to the maximum length. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cfe7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(original_indices_list: list, maximum_length=100) -> list: \n",
    "    \"\"\"\n",
    "    Truncates the original_indices_list to the maximum_length\n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0ba3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your truncate works on this toy example \n",
    "toy_example = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "truncate(toy_example, maximum_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5729ee",
   "metadata": {},
   "source": [
    "Now let's pad. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b19945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add <PAD> symbol (also as embedding for the word type \"the\")\n",
    "new_pad_entry = len(idx2vocab)\n",
    "idx2vocab += [\"<PAD>\"]\n",
    "vocab2indx[\"<PAD>\"] = new_pad_entry\n",
    "embed_array_w_oov_pad = add_the_embedding(embed_array_w_oov, vocab2indx)\n",
    "print(\"Pad index = \", new_pad_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecff0d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(original_indices_list: list, pad_index: int, maximum_length=100) -> list: \n",
    "    \"\"\"\n",
    "    Given original_indices_list, concatenates the pad_index enough times \n",
    "    to make the list to maximum_length. \n",
    "    \"\"\"\n",
    "    # TODO: implement your solution here \n",
    "    # CODE START\n",
    "    raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "    # CODE END "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7e9ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your implementation adds the pad index here \n",
    "toy_example1 = [1, 2, 3, 4]\n",
    "pad(toy_example1, new_pad_entry, maximum_length=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0646cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that your implementation DOES NOT adds the pad index here \n",
    "toy_example2 = [1, 2, 3, 4, 5]\n",
    "pad(toy_example2, new_pad_entry, maximum_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f008221e",
   "metadata": {},
   "source": [
    "Let's now put all our processing together for one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e9ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_toks = train_examples[5]\n",
    "print(\"Training example 1 tokens = \", train1_toks)\n",
    "print()\n",
    "print(\"Trianing example 1 length =\", len(train1_toks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdedcff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_indices = create_word_indices(train1_toks, vocab2indx)\n",
    "print(len(train1_indices))\n",
    "print(\"First five toks =\", train1_indices[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d45bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that converting back we get the correct word back  \n",
    "idx2vocab[192]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7bbc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check your truncation is working correctly! \n",
    "train1_truncated = truncate(train1_indices, maximum_length=4)\n",
    "print(train1_truncated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c137fd",
   "metadata": {},
   "source": [
    "Now we can convert all our training and dev examples. No need to modify the cells below, just run them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbe1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to modify this function \n",
    "def convert_X(examples):\n",
    "    MAXIMUM_LENGTH = 25\n",
    "    \n",
    "    X_list = []\n",
    "    for one_train_example in examples: \n",
    "        one_train_indices = create_word_indices(one_train_example, vocab2indx)\n",
    "        one_train_indices = truncate(one_train_indices, maximum_length=MAXIMUM_LENGTH)\n",
    "        one_train_indices = pad(one_train_indices, new_pad_entry, maximum_length=MAXIMUM_LENGTH)\n",
    "        X_list.append(one_train_indices)\n",
    "        \n",
    "    X = torch.LongTensor(X_list)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3932ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convert_X(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b4eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "assert X_train.shape[0] == len(train_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cabe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dev = convert_X(dev_examples)\n",
    "print(X_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = torch.LongTensor(train_labels)\n",
    "assert Y_train.shape[0] == X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c77a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_dev = torch.LongTensor(dev_labels)\n",
    "assert Y_dev.shape[0] == X_dev.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5e69b4",
   "metadata": {},
   "source": [
    "#### Implementing the network \n",
    "\n",
    "It's now time to implement the network in Pytorch! \n",
    "\n",
    "Deep learning networks can be notoriously hard to debug. Here's some **suggestions** to help you: \n",
    "- Work out on paper what you're supposed to do given the description at the start of Part 2D. - Check-in with your classmates about high-level approaches and concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d4b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Big hint: This is how you should use the frozen embedding layer\n",
    "# Make sure you understand what the code is doing in this cell and the next one \n",
    "pretrained_embedding_matrix = embed_array_w_oov_pad\n",
    "vecs = torch.FloatTensor(pretrained_embedding_matrix)\n",
    "embed = nn.Embedding.from_pretrained(vecs, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80aca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two toy examples that we'll pass through the embedding layer. \n",
    "# Each example has 3 tokens which we converted into indices. \n",
    "toy2_input_indices = [[2548, 2876, 37], [3, 4, 5]]\n",
    "toy2_input_tensor  = torch.LongTensor(toy2_input_indices)\n",
    "toy2_embedded = embed(toy2_input_tensor)\n",
    "print(toy2_embedded.shape)\n",
    "assert toy2_embedded.shape[0] == len(toy2_input_indices) # number of example \n",
    "assert toy2_embedded.shape[1] == len(toy2_input_indices[0]) #number of tokens (will be equal to MAX LENGTH)\n",
    "assert toy2_embedded.shape[2] == 50 # embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122c627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy2_embedded[0][0] #first example, first token is a 50-dimensional embdding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d42cfc7",
   "metadata": {},
   "source": [
    "Implement `__init__()` and `forward()` in the network below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602aacb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAveragingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Pytorch implementation for Deep Averaging Network for classification \n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, #number of labels / y-values\n",
    "                     pretrained_embedding_matrix, #we'll pass in embed_array_w_oov_pad\n",
    "                     embedding_dim: int, #architecture/pre-processing decision \n",
    "                     hidden_dim1: int, #architecture decision\n",
    "                     hidden_dim2: int, #architecture decision \n",
    "                     leaky_relu_negative_slope: float, #hyperparameter\n",
    "                     dropout_probability: float #hyperparameter\n",
    "                ):\n",
    "        \"\"\"\n",
    "        Create the network architecture. \n",
    "        \n",
    "        Hints: \n",
    "        - Make sure all your dimesions of various layers work out correctly \n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes \n",
    "        \n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END \n",
    "        \n",
    "        \n",
    "    def forward(self, X_batch: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Given X_batch, make the forward pass through the network. \n",
    "        \n",
    "        The output should be the predicted *log probabilities*. \n",
    "        \n",
    "        Returns: \n",
    "            - (torch.Tensor): the log probabilites after the forward pass \n",
    "                The shape of this tensor should be (X_batch.shape[0], 2)\n",
    "                \n",
    "        Hints: \n",
    "            - Look at Pytorch's implemenation of .mean()\n",
    "            - There should be NO for-loops in this method \n",
    "        \"\"\"\n",
    "        # TODO: implement your solution here \n",
    "        # CODE START\n",
    "        raise NotImplementedError(\"Solution not yet implemented!\") #delete this line and add your solution\n",
    "        # CODE END\n",
    "        \n",
    "    def train_model(self, X_train, Y_train, X_dev, Y_dev, loss_fn, optimizer, num_iterations, batch_size = 500, check_every=10, verbose=False): \n",
    "        \"\"\"\n",
    "        Method to train the model. \n",
    "        \n",
    "        No need to modify this method. \n",
    "        \"\"\"\n",
    "        self.train() # tells nn.Module its in training mode \n",
    "                      # (important when we get to things like dropout)\n",
    "            \n",
    "        loss_history = [] #We'll record the loss for inspection\n",
    "        train_accuracy = []\n",
    "        dev_accuracy = []\n",
    "\n",
    "        for t in range(num_iterations):\n",
    "            if batch_size >= X_train.shape[0]: \n",
    "                X_batch = X_train\n",
    "                Y_batch = Y_train\n",
    "            else: #randomly choose batch_size number of examples \n",
    "                batch_indices = np.random.randint(X_train.shape[0], size=batch_size)\n",
    "                X_batch = X_train[batch_indices]\n",
    "                Y_batch = Y_train[batch_indices]\n",
    "\n",
    "            \n",
    "            # Forward pass \n",
    "            pred = self.forward(X_batch)\n",
    "            loss = loss_fn(pred, Y_batch)\n",
    "\n",
    "            #Backprop\n",
    "            optimizer.zero_grad() # clears the gradients from the previous iteration\n",
    "                                  # this step is important because otherwise Pytorch will \n",
    "                                  # *accumulate* gradients for all itereations (all backwards passes)\n",
    "            loss.backward() # calculate gradients from forward step \n",
    "            optimizer.step() # gradient descent update equation \n",
    "            \n",
    "            #Check the loss and train and dev accuracies every \n",
    "            if t % check_every == 0:\n",
    "                loss_value = loss.item() # call .item() to detach from the tensor \n",
    "                loss_history.append(loss_value)\n",
    "                \n",
    "                #Check train accuracy (entire set, not just batch) \n",
    "                train_y_pred, _ = self.predict(X_train)\n",
    "                train_acc = self.accuracy(train_y_pred, Y_train.detach().numpy()) \n",
    "                train_accuracy.append(train_acc)\n",
    "                \n",
    "                #Check dev accuracy (entire set, not just batch) \n",
    "                dev_y_pred, _ = self.predict(X_dev)\n",
    "                dev_acc = self.accuracy(dev_y_pred, Y_dev.detach().numpy())\n",
    "                dev_accuracy.append(dev_acc)\n",
    "                \n",
    "                if verbose: print(f\"Iteration={t}, Loss={loss_value}\")\n",
    "                \n",
    "        return loss_history, train_accuracy, dev_accuracy\n",
    "    \n",
    "    def predict(self, X): \n",
    "        \"\"\"\n",
    "        Method to make predictions given a trained model. \n",
    "        \n",
    "        No need to modify this method. \n",
    "        \"\"\"\n",
    "        self.eval() # tells nn.Module its NOT in training mode \n",
    "                 # (important when we get to things like dropout)\n",
    "    \n",
    "        pred_log_probs = self.forward(X)\n",
    "        \n",
    "        \n",
    "        if self.num_classes == 2: \n",
    "            log_pred_pos_class = pred_log_probs[:,1].detach().numpy() #get only the positive class \n",
    "            pred_pos_class = np.exp(log_pred_pos_class) #exp to undo the log \n",
    "            # decision threshold\n",
    "            y_pred = np.zeros(X.shape[0])\n",
    "            y_pred[pred_pos_class>= 0.5] = 1\n",
    "            return y_pred, pred_pos_class\n",
    "        \n",
    "        else: \n",
    "            return pred_log_probs\n",
    "    \n",
    "    @staticmethod\n",
    "    def accuracy(y_pred: np.ndarray, y_true: np.ndarray) -> float: \n",
    "        \"\"\"\n",
    "        Calculates accuracy. No need to modify this method. \n",
    "        \"\"\"\n",
    "        return np.mean(y_pred == y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07271293",
   "metadata": {},
   "source": [
    "**Sanity check.** A good first step for debugging deep learning networks is to create a random `X_batch` and check that the syntax of the network is correct and the outputs are the correct shape and type. Run the cells below to perform this sanity check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b90414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random inputs to check syntax\n",
    "X_batch = torch.randint(low=0, high=len(vocab2indx), size=(100, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5480328",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3f2494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize model and run the forward pass \n",
    "model = DeepAveragingNetwork(2, embed_array_w_oov_pad,50, 20, 20, 0.01,0.1)\n",
    "log_probs_out = model.forward(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea17496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cells below and make sure your network passes the checks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69548f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(log_probs_out) == torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d038b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert log_probs_out.shape == (X_batch.shape[0], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d28bffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all your probabilities add to 1\n",
    "log_probs_numpy = log_probs_out.detach().numpy()\n",
    "probs = np.exp(log_probs_numpy)\n",
    "np.sum(probs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e9f60",
   "metadata": {},
   "source": [
    "**Training the full network.** Let's now run the network on real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-1\n",
    "HIDDEN_DIM1 = 200 \n",
    "HIDDEN_DIM2 = 100\n",
    "LEAKY_RELU_NEG_SLOPE = 0.01\n",
    "DROPOUT_PROB = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34de8427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initalize\n",
    "model = DeepAveragingNetwork(2,\n",
    "                         embed_array_w_oov_pad,\n",
    "                         50,\n",
    "                         HIDDEN_DIM1, \n",
    "                         HIDDEN_DIM2, \n",
    "                         LEAKY_RELU_NEG_SLOPE,\n",
    "                         DROPOUT_PROB) \n",
    "\n",
    "loss_fn= nn.NLLLoss() #For binary logistic regression \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE) #stochastic gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974e85d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_ITERATIONS = 300\n",
    "loss_history, train_accuracy, dev_accuracy = model.train_model(X_train, Y_train, X_dev, Y_dev, \n",
    "                                                             loss_fn, optimizer, NUMBER_ITERATIONS, \n",
    "                                                             batch_size = 200,\n",
    "                                                             check_every=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69de3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.diagnostics_plot(loss_history, train_accuracy, dev_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Baseline acc (predict majority class) =\", np.mean(dev_labels))\n",
    "print(\"Final train acc = \", train_accuracy[-1])\n",
    "print(\"Final dev acc =\", dev_accuracy[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb7cb36",
   "metadata": {},
   "source": [
    "There are many \"engineering\" pieces to try to make these accuracies better. If you're interested, we encourage you to try the extra credit below! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc4800b",
   "metadata": {},
   "source": [
    "### (Optional) 3. Extra credit \n",
    "*Extra credit can only help you and will not hurt you. At the end of the semester, if you have a borderline grade, your grade could be increased given your efforts on extra credit. This section is intended to be open-ended and challenge you. We suggest you only attempt this section after you have completed all other parts and are satisifed with your submission.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df449d70",
   "metadata": {},
   "source": [
    "Try to get a higher accuracy on the Yelp data using the Deep Averaging Network you implemented. Some suggestions: \n",
    "- Try a grid search over the hyperparameters. \n",
    "- Implement a [random layout rather than a grid layout](https://towardsdatascience.com/hyper-parameter-tuning-techniques-in-deep-learning-4dad592c63c8) for hyperparameter search. \n",
    "- Use the `gensim` package and use different pre-trained embeddings (try to get fewer OOVs).\n",
    "- Change other preprocessing decisions (hint we implemented some decisions in capital letters; find these and change them). \n",
    "- Change the model architecture (e.g. try an LSTM or a transformer).\n",
    "- Change the optimizer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9a9df9",
   "metadata": {},
   "source": [
    "**Instructions for extra credit submission.**\n",
    "Were separating the extra credit from the normal submission so that (1) your extra credit does not affect your normal submission and (2) we do not break the memory limits on the Gradescope autograder.\n",
    "\n",
    "To sumbit: \n",
    "1. Create a new jupyter notebook (.ipynb) file.\n",
    "2. Write all your extra credit in this file.\n",
    "3. Once youre done, in the top menu bar make sure to `Kernel -> Restart -> RunAll`.\n",
    "4. In the top menu bar, select` File -> Download as -> PDF via Latex (.pdf)`\n",
    "5. Upload this `.pdf` to Gradescope under the appropriate extra credit assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5685f5fa",
   "metadata": {},
   "source": [
    "## Submission "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5490e027",
   "metadata": {},
   "source": [
    "**Processing reporting.** Please record your answers to the questions below by writing directly in this Markdown cell.\n",
    "\n",
    "If you talked with any of your classmates on this assignment please list their names here:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*\n",
    "\n",
    "Approximately how much time did you spend on this assignment:\n",
    "\n",
    "*DELETE AND PUT YOUR ANSWER HERE.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900d5fe5",
   "metadata": {},
   "source": [
    "**Download zip.** Once you're satsified with your solution, save this file and run the cell below to automatically zip your file. This will produce `submission.zip` in the same folder as this file (same folder as `hw5.ipynb`). \n",
    "\n",
    "Submit `submission.zip` to Gradescope. \n",
    "\n",
    "*Note:* This script assumes that you have the `zip` utility installed and you can use `bash` on your system. If the cell below does not work you may need to zip your file manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10051abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "if [[ ! -f \"./hw5.ipynb\" ]]\n",
    "then\n",
    "    echo \"WARNING: Did not find notebook in Jupyter working directory. Manual solution: go to File->Download .ipynb to download your notebok and other files, then zip them locally.\"\n",
    "else\n",
    "    echo \"Found notebook file, creating submission zip...\"\n",
    "    zip -r submission.zip hw5.ipynb\n",
    "fi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cs375] *",
   "language": "python",
   "name": "conda-env-cs375-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
